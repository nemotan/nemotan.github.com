---
layout: post
title: hiev
categories:
- hive
tags:
- hive
---
##hadoop
hadoop作业提交shell：
	
	hadoop jar $home/03ws/sp_intel/out/artifacts/hadoop_behavior_jar/hadoop_behavior.jar com.landray.behavior.job.JobManager
##hive分区表设计

方案一：每一天存放一个目录，每天生成一个日志文件（所有用户），并且跨天的日志


-	request_transform
   -  2015-09
 		- 2015-09-07（文件传输时间）

方案二：每个用户一个主文件夹，会导致小文件很多，好维护需要每个月归档，但是有个问题就是面对跨天的问题，归档会错乱

- user_id 		
	- month
		- day
 
##hiveserver2和beeline客户端

1、服务端启动<br>
	
	hive --service hiveserver2或者直接：hiveserver2
2、客户端连接
	
	在hadoop集群的机器上直接输入：
	远程连接：需要hive和hadoop复制客户端机器，并配置好变量
	shell链接：beeline -u jdbc:hive2://master:10000 -n nemo -p tan -e "select count(*) from request_log"
	beelilne客户端链接：
	输入beeline后，!connect jdbc:hive2://localhost:10000
##hive jdbc客户端开发环境搭建步骤
1、创建一个mavenmodule<br>
2、引入必要jar包

{% highlight java %}

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>2.6.0</version>
</dependency>
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <version>1.2.1</version>
</dependency>
{% endhighlight %}
	
	
 3、jdbc连接
 
{% highlight java %}
 try {
             Class.forName(driverName);
        } catch (ClassNotFoundException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
            System.exit(1);
        }
        //replace "hive" here with the name of the user the queries should run as
        Connection con = DriverManager.getConnection("jdbc:hive2://master:10000/default", "nemo", "lovelili");
        Statement stmt = con.createStatement();
        String tableName = "testHiveDriverTable";
        stmt.execute("drop table if exists " + tableName);
{% endhighlight %}

 	 
   
##hive自定义函数UDAF实例
**参考：**<a href="http://www.cnblogs.com/ggjucheng/archive/2013/02/01/2888051.html">hive udaf开发入门和运行过程详解</a><br>
**常用命令:**<br>

{% highlight bash %}
#打包：
javac -d Lower Lower.java
jar -cvf collect_size.jar -C / .
使用mvn打包：mvn package
#临时注册
I;
create temporary function collect_set_size as 'com.landray.hive.ql.GenericUDAFCollectSetSize';
	create temporary function size_sum as 'com.landray.hive.ql.ArraySizeSum';
#永久注册
org.apache.hadoop.hive.ql.exec.FunctionRegistry源码见这个
show functions	#显示所有的函数	
list jars		#显示所有的临时jars
{% endhighlight %}
##hive常用命令
hive常用ddl：

	netstat –apn | grep 10000
	ps -aux | grep pid
	
	show tables #显示所有的表
	desc table_name #查询表结构
	show partitions request_log; #显示分区信息
	select url['server'] from request_log where dt='1797'; #查询map结构
	select ref,size(ref)from request_log; #查询map类型的size可以判断size是否为空 
	select * from request_log limit 1; #top查询
	!connect jdbc:hive2://localhost:10000 #belline客户端链接server端口
	beeline -u jdbc:hive2://localhost:10000 -n nemo -p tan -e "show tables"; #客户端连接
	
hive创建外部表：
	
	CREATE EXTERNAL TABLE IF NOT EXISTS request_log(
	file_date STRING,
	id STRING,
	job_create BIGINT,
	node STRING,
	session_id STRING,
	time BIGINT,
	ip STRING,
	user_id STRING,
	name STRING,
	ua INT,
	browser STRING,
	browser_ver STRING,
	log_type STRING,
	dt BIGINT,
	url MAP<STRING,STRING>,
	ref MAP<STRING,STRING>)
	PARTITIONED BY (month STRING,day STRING)
	ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
	COLLECTION ITEMS TERMINATED BY ','
	MAP KEYS TERMINATED BY '::'
	LOCATION '/output/logs/request_transform/';
增加分区：

	_month='2015-09'
	_date='2015-09-07'
	hive -e "ALTER TABLE request_log ADD PARTITION(month='${_month}',day='${_date}') LOCATION '/output/logs/request_tra
	nsform/${_month}/${_date}';"
	
##日志记录步骤：
###原始日志：
	1（标记）	6F1F682FD889EAEE10EEF84769CBD0F9（服务器段sessionId）	1432137603417(记录日志|请求之后的当前时间)	183.13.217.114(ip地址)	1271da54250e08909144e724b32b689c（用户Id）	%E7%8E%8B%E5%8D%8E(用户名加密)	6（客户端类型）【浏览器】 【浏览器版本】			html【contentType】	/sys/task/sys_task_main/sysTaskMain.do?method=addChildTask&fdTaskId=14d1e27ce81307c522a09014402a1102&flag=	/sys/task/sys_task_main/sysTaskMain.do?method=view&fdId=14d1e27ce81307c522a09014402a1102&_mobile=1【URI并加上参数】【refer url】	1797【时间差】
###清理日志：
	file_year					string       文件年
	file_month				string    	文件月
	file_date           	string    	文件day
	id                  	string			项目Id
	job_create          	bigint			job创建时间
	node                	string			根据文件名获取nodename
	session_id          	string			sessionId
	time                	bigint			记录时间-时间差=请求开始时间
	ip                  	string			ip地址
	user_id             	string			userid
	name              		string			用户名
	ua                  	int				MobileUiil中的UA
	browser             	string			根据UA得出的浏览器
	browser_ver         	string			根据UA得出的浏览器版本
	log_type            	string			contentTYPE得出的类型
	dt                  	bigint			请求时间差
	url                 	map<string,string>	url清洗之后
	ref                 	map<string,string>	ref清洗之后
	isModule					boolean		清洗，判断日志是否可以
	month               	string			分区|日志调度month
	day                 	string			分区|日志调度day
<br>
<br>
<br>	
##request日志分析
###数据经过M-R清洗之后存入hive
**验证：**总数验证
###表结构：
	
	
##客户端数据分析	
**创建临时表**

	create table if not exists table_client_2015_09_07_tmp (id string,file_month string,count bigint,v_type string,v_value string,v_version string);	
**系统总访问人数：**需要对用户ID进行去重
	
	select count(distinct user_id) from request_log where size(ref)=0 or ref['server']!=url['server'];
AL
	最终：
	hive -e "insert into table_client_${_date}_tmp select id,file_month,count(distinct user_id),'*','*','*' from request_log where (size(ref)=0 or ref['server']!=url['server']) and file_month in (select distinct file_month from request_log) group by id,file_month";
	
	
###<font color="red">PC端数A据：</font>
**所有PC端：**
	
	hive -e "insert into table_client_${_date}_tmp select id,file_month,count(distinct user_id),'PC','*','*' from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua=-1 and file_month in (select distinct file_month from request_log) group by id,file_month";
**PC端浏览器：**
	
	hive -e "select id,file_month,count(distinct user_id),'PC', browser,'*' from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua=-1 and browser !='' and file_month in (select distinct file_month from request_log) group by id,file_month,browser";
		
**PC段浏览器版本：**
	
	hive -e "select id,file_month,count(distinct user_id),'PC',browser,browser_ver from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua=-1 and browser !='' and browserVer!='' and file_month in (select distinct file_month from request_log) group by id,file_month,browser,browser_ver";
	
###<font color="red">移动端手机类型分析</font>
**所有移动端:**
	
	hive -e "select id,file_month,count(distinct user_id),'PHONE','*','*' from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and file_month in (select distinct file_month from request_log) group by id,file_month"
	
**移动端手机类型：**使用子查询
		
	select phone,sum(count_each) from (select case 
		when ua=1 then 'iphone'
	    when ua=2 then'iphone'
	    when ua=3 then 'iphone'
	    when ua=4 then 'android'
	    when ua=5 then 'android'
	    when ua=7 then 'iphone'
	    when ua=8 then 'android'
        when ua=9 then 'iphone'
	    when ua=10 then 'android' end as phone,count(distinct user_id) from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and ua !=0 and ua !=6 and ua !=11 group by ua)a group by phone;
	#0,6,11代表web、微信、钉钉不纳入手机类型统计
	
	
		
	select phone,sum(count_each) from (select case when ua=1 then 'iphone' when ua=2 then'iphone' when ua=3 then 'iphone' when ua=4 then 'android' when ua=5 then 'android' when ua=7 then 'iphone' when ua=8 then 'android' when ua=9 then 'iphone' when ua=10 then 'android' end as phone,count(distinct user_id) as count_each from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and ua !=0 and ua !=6 and ua !=11 group by ua)a group by phone;
	
	select a.phone,count(distinct a.user_id) from (select case when ua=1 then 'iphone' when ua=2 then'iphone' when ua=3 then 'iphone' when ua=4 then 'android' when ua=5 then 'android' when ua=7 then 'iphone' when ua=8 then 'android' when ua=9 then 'iphone' when ua=10 then 'android' end as phone,user_id from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 group by ua,user_id)a group by a.phone;
	
	hive-sql最终版:
	hive -e "select a.id,a.file_month,count(distinct a.user_id),'PHONE',a.phone,'*' from (select id,file_month, case when ua=1 then 'iphone' when ua=2 then'iphone' when ua=3 then 'iphone' when ua=4 then 'android' when ua=5 then 'android' when ua=7 then 'iphone' when ua=8 then 'android' when ua=9 then 'iphone' when ua=10 then 'android' end as phone,user_id from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and file_month in (select distinct file_month from request_log) group by id,file_month,ua,user_id)a group by a.id,a.file_month,a.phone"
	
###<font color="red">移动端入口分析</font>
**所有入口:**
	
	hive -e "select id,file_month,count(distinct user_id),'ENTRY','*','*' from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and file_month in (select distinct file_month from request_log) group by id,file_month";
	


**入口类型：**

	
	查询方案一：先按照ua分组再查询出ua得数量，再按照类型入口分组（错误）
	select entry,sum(count_each) from (select case when ua=0 then 'WEB' when ua=1 then 'WEB' when ua=2 then 'EKP' when ua=3 then 'EKP' when ua=4 then 'EKP' when ua=5 then 'EKP' when ua=6 then 'WEIXIN' when ua=7 then 'kk4' when ua=8 then 'kk4' when ua=9 then 'kk5' when ua=10 then 'kk5' when ua=11 then 'DING' end as entry,count(distinct user_id) as count_each from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 group by ua)a group by entry;
	

	#查询结果：
	0	3
	4	1
	6	1
	7	30
	8	35
	
	EKP	1
	WEB	3
	WEIXIN	1
	kk4	65
	7个8表示KK4，按照这种查询可能7和8中的userID重复，因此该数据可能会多
	
	查询方案二：
	hive -e "select a.id,a.file_month,count(distinct a.user_id),'ENTRY',a.entry,'*' from (select id,file_month,case when ua=0 then 'WEB' when ua=1 then 'WEB' when ua=2 then 'EKP' when ua=3 then 'EKP' when ua=4 then 'EKP' when ua=5 then 'EKP' when ua=6 then 'WEIXIN' when ua=7 then 'kk4' when ua=8 then 'kk4' when ua=9 then 'kk5' when ua=10 then 'kk5' when ua=11 then 'DING' end as entry,user_id from request_log where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and file_month in (select distinct file_month from request_log) group by id,file_month,ua,user_id)a group by a.id,a.file_month,a.entry”
	
	查询方案三：由于方案二种有count(distinct)会导致所有的数据传入到一个reduce任务中从而影响效率，因此给出优化之后的方案三。
	
	select b.id,b.file_month,count(*),'ENTRY','*' from (select a.id as id,a.file_month as file_month,a.entry as entry,a.user_id from (select id,file_month,user_id,case when ua=0 then 'WEB' when ua=1 then 'WEB' when ua=2 then 'EKP' when ua=3 then 'EKP' when ua=4 then 'EKP' when ua=5 then 'EKP' when ua=6 then 'WEIXIN' when ua=7 then 'kk4' when ua=8 then 'kk4' when ua=9 then 'kk5' when ua=10 then 'kk5' when ua=11 then 'DING' end as entry from w where (size(ref)=0 or ref['server']!=url['server']) and ua>=0 and ua <=11 and month='2015-09')a group by a.id,a.file_month,a.entry,a.user_id)b group by b.id,b.file_month,b.entry
	 

		
	#结果：
	EKP	1
	WEB	3
	WEIXIN	1
	kk4	46


##System_load

sql语句bak:

	#取整数
	select floor(time/3600000) from request_log limit 1;
	
	#按照计算之后的分组
	select floor(time/3600000) as time from request_log group by node,floor(time/3600000);
	
	#测试自定义函数
	select node,floor(time/3600000), collect_set_size(dt) as maxTime,count(*) as count,size_sum(maxTime,2) from request_log group by node,floor(time/3600000);
	
	#完整shell
	#!/bin/bash
	countEx="round(count(*)*0.05)"
	maxTime="collect_set_size(dt)"
	index="case when $countEx<=size($maxTime) then $countEx when $countEx>size($maxTime) then size($maxTime) end"
	sumEx="size_sum($maxTime,$index)"
	
	hive -e "select node,floor(time/3600000),collect_set_size(dt) as maxTime,count(*) as count,round(sum(dt)-$sumEx)/(count(*)-$index) as avg from request_log group by node,floor(time/3600000)"
	
	生成的sql：
	select node,floor(time/3600000), collect_set_size(dt) as maxTime,count(*) as count,round(sum(dt)-size_sum(collect_set_size(dt),case when round(count(*)*0.05)<=size() then round(count(*)*0.05) when round(count(*)*0.05)>size(collect_set_size(dt)) then size(collect_set_size(dt)) end))/(count(*)-case when round(count(*)*0.05)<=size(collect_set_size(dt)) then round(count(*)*0.05) when round(count(*)*0.05)>size(collect_set_size(dt)) then size(collect_set_size(dt)) end) as avg from request_log group by node,floor(time/3600000)

	测试sql：
	select node,floor(time/3600000), collect_set_size(dt) as maxTime,count(*) as count,min(2,20) as avg from request_log group by node,floor(time/3600000);
	
	select node,floor(time/3600000), collect_set_size(dt) as maxTime,count(*) as count,sum(dt),case when round(count(*)*0.05)<=size(collect_set_size(dt)) then round(count(*)*0.05) when round(count(*)*0.05)>size(collect_set_size(dt)) then size(collect_set_size(dt)) end,round(count(*)*0.05) from request_log group by node,floor(time/3600000)
	
	
	select node,round(time/3600000),collect_set_size(dt) from request_log group by node,round(time/3600000);

{% highlight bash %}
#调错
select id as id,   	 file_month as scop,	 node as node,	 floor(time/3600000) as time,	 count(*) as count,	 sum(dt) as sumTime,	 collect_set_size(dt) as maxTime,	 round(sum(dt)-size_sum(collect_set_size(dt),case when round(count(*)*0.05)<=size(collect_set_size(dt)) then round(count(*)*0.05) when round(count(*)*0.05)>size(collect_set_size(dt)) then size(collect_set_size(dt)) end))/(count(*)-case when round(count(*)*0.05)<=size(collect_set_size(dt)) then round(count(*)*0.05) when round(count(*)*0.05)>size(collect_set_size(dt)) then size(collect_set_size(dt)) end) as avgTime from request_log   where day='2015-09-07' group by id,file_month,node,floor(time/3600000)

select id as id,file_month as scop,node as node,round(time/3600000) as time,count(*) as count from request_log where day='2015-09-07' group by id,file_month,node,round(time/3600000)

{% endhighlight %}	
	
 
##time_cost
sql语句bak：

    #未使用avg函数
	select id,file_month,url['module'],url['method'],count(*) as count,max(case when dt>=100000 then floor(dt/100000)*100+100  when dt>=10000 and dt<100000 then floor(dt/10000)*10+10 when dt<10000 then floor(dt/1000)+1 end) as maxTime,sum(case when dt>=100000 then floor(dt/100000)*100+100  when dt>=10000 and dt<100000 then floor(fodt/10000)*10+10 when dt<10000 then floor(dt/1000)+1 end) as sum,round(sum(case when dt>=100000 then floor(dt/100000)*100+100  when dt>=10000 and dt<100000 then floor(dt/10000)*10+10 when dt<10000 then floor(dt/1000)+1 end)/count(*)) from request_log where month='2015-09' group by id,file_month,url['module'],url['path'],url['method'] order by sum desc;
	
	#avg函数	
	select id,file_month,url['module'],url['method'],count(*) as count,max(case when dt>=100000 then floor(dt/100000)*100+100  when dt>=10000 and dt<100000 then floor(dt/10000)*10+10 when dt<10000 then floor(dt/1000)+1 end) as maxTime,sum(case when dt>=100000 then floor(dt/100000)*100+100  when dt>=10000 and dt<100000 then floor(dt/10000)*10+10 when dt<10000 then floor(dt/1000)+1 end) as sum,round(avg(case when dt>=100000 then floor(dt/100000)*100+100  when dt>=10000 and dt<100000 then floor(dt/10000)*10+10 when dt<10000 then floor(dt/1000)+1 end)) from request_log where month='2015-09' group by id,file_month,url['module'],url['path'],url['method'] order by sum desc;	
##module_count
- 按照module和path进行查询

{% highlight bash %}
select 'path',id,file_month,url['module'],url['path'],url['method'],case when ua is null or ua=-1 then 'PC'    	    when ua=0 or ua=1 then 'WEB' 	    when ua>=2 and ua <=5 then 'EKP' 	    when ua=6 then 'WEIXIN' 	    when ua>=7 and ua<=10 then 'KK' 	    when ua>=11 then 'UNKNOW' end,count(*) from request_log group by id,file_month,url['module'],url['path'],url['method'],case when ua is null or ua=-1 then 'PC'   	    when ua=0 or ua=1 then 'WEB' 	    when ua>=2 and ua <=5 then 'EKP' 	    when ua=6 then 'WEIXIN' 	    when ua>=7 and ua<=10 then 'KK' 	    when ua>=11 then 'UNKNOW' end


add jar /home/nemo/hive-1.2.1/lib/hive_behavior.jar;
create temporary function collect_set_size as 'com.landray.hive.ql.GenericUDAFCollectSetSize';
create temporary function check as 'com.landray.hive.ql.GenericUDFCheck';

select check url['module'] is not null and (log_type is null or log_type='html') and url['portlet']='false'

select * from request_log where url['module'] is not null and (log_type is null or log_type='html') and url['portlet']='false';

select * from request_log where url['module'] is not null and (log_type is null or log_type='html') and url['portlet']='false' and url['method']='add';
{% endhighlight %}
- 进行行列翻转

{% highlight bash %}
 #行列倒置，补全ALL,PC,WEB,EKP,WEIXIN,KK等列
  REVERSE_HQL="select a.id,a.scop,a.type,a.module,a.path,a.method,\
               sum(a.count) as sum,\
               max(case when a.ua='PC' then a.count else 0 end),\
               max(case when a.ua='WEB' then a.count else 0 end),\
               max(case when a.ua='EKP' then a.count else 0 end),\
               max(case when a.ua='WEIXIN' then a.count else 0 end),\
               max(case when a.ua='KK' then a.count else 0 end)\
               from ($HQL_PATH UNION ALL $HQL_MODULE)a \
               group by a.id,a.scop,a.type,a.module,a.path,a.method"
{% endhighlight %}	
	
- 创建临时表存储结果

{% highlight bash %}
create table module_2015-09-07_tmp()
{% endhighlight %}
- 结果导入到mongodb


##暴露的问题
1、job提交前宕机问题<br>
2、hive并行，shell并发进程卡机问题<br>
3、hive数据倾斜问题<br>参考：http://blog.itpub.net/29754888/

count(distinct)性能问题分析
viewspace-1262946/
hive.map.aggr=true
hive.groupby.skewindata=true
4、多job运行问题<br>

http://blog.csdn.net/lpxuan151009/article/details/7956518源码分析


#附录
###hadoop、hive以及hive常用的命令：
{% highlight bash %}	
hadoop jar $home/03ws/sp_intel/out/artifacts/hadoop_behavior_jar/hadoop_behavior.jar com.landray.behavior.job.JobManager #hadoop提交作业

jar xf xxx.jar   #解压jar包 输入jar命令会有详细提示
jar cvf hive_behavior.jar com/ #把包下所有的class文件打包，com也打进去
sudo du -shm /Users/nemo/Library/* | sort -nr #查看文件夹下的大小
sh module.sh file_month month 2015-09-07 #执行某个
{% endhighlight %}	
**创建表语句**

{% highlight bash %}
CREATE table student_map as select * from student;
CREATE TABLE IF NOT EXISTS student_map(id MAP<STRING,STRING>,value MAP<STRING,STRING>)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'; #创建MAP类型的表
insert into student_map as 
{% endhighlight %}

###vim常用命令
{% highlight bash %}
set nu #显示行号
$ #定位到行末尾
0 #定位到行首
nl #移动到n列之后

{% endhighlight %}

###苹果系统清理bash
{% highlight bash %}
sudo rm -fr /Users/nemo/Library/Caches #清理应用缓存文件
sudo rm -fr /Users/nemo/Library/Containers/com.tencent.qq #移除qq缓存
{% endhighlight %}

###bak
{% highlight bash %}
  #执行hive
  hive -e "$HQL_SYS_ALL\
           UNION ALL $HQL_PC\
           UNION ALL $HQL_PC_BRO\
           UNION ALL $HQL_PC_BRO_VER\
           UNION ALL $HQL_MOBILE\
           UNION ALL $HQL_PHONE\
           UNION ALL $HQL_ENTRY\
           UNION ALL $HQL_ENTRY_TYPE\
           "
{% endhighlight %}	

###mongodb常用查询语句

{% highlight bash %}
db.getCollection('count_module_201505').find({"_id.type":"module"},{"_id.module":1})

select ref['module'],ref['path'],url['method'] from request_log where url['module'] is not null       and (log_type is null or log_type='html')        and url['portlet']='false'        and check(url['module'],url['path'],url['method'])=true and day='2015-09-07';

select distinct url['module'],url['path'],url['method'] from request_log where url['module'] is not null       and (log_type is null or log_type='html')        and url['portlet']='false'        and check(url['module'],url['path'],url['method'])=true and day='2015-09-07';
{% endhighlight %}

###坑

{% highlight java %}
1、count_module查询的时候不准确
	--过滤数据null判断要判断字符串
	--要忽略大小写判断

{% endhighlight %}
###hive调优

{% highlight java %}
1、join连接时的优化：当三个或多个以上的表进行join操作时，如果每个on使用相同的字段连接时只会产生一个mapreduce。
2、join连接时的优化：当多个表进行查询时，从左到右表的大小顺序应该是从小到大。原因：hive在对每行记录操作时会把其他表先缓存起来，直到扫描最后的表进行计算
3、在where字句中增加分区过滤器。
4、当可以使用left semi join 语法时不要使用inner join，前者效率更高。原因：对于左表中指定的一条记录，一旦在右表中找到立即停止扫描。
5、如果所有表中有一张表足够小，则可置于内存中，这样在和其他表进行连接的时候就能完成匹配，省略掉reduce过程。设置属性即可实现，set hive.auto.covert.join=true; 用户可以配置希望被优化的小表的大小 set hive.mapjoin.smalltable.size=2500000; 如果需要使用这两个配置可置入$HOME/.hiverc文件中。
6、同一种数据的多种处理：从一个数据源产生的多个数据聚合，无需每次聚合都需要重新扫描一次。
例如：insert overwrite table student select *　from employee; insert overwrite table person select * from employee;
可以优化成：from employee insert overwrite table student select * insert overwrite table person select *
7、limit调优：limit语句通常是执行整个语句后返回部分结果。set hive.limit.optimize.enable=true;
8、开启并发执行。某个job任务中可能包含众多的阶段，其中某些阶段没有依赖关系可以并发执行，开启并发执行后job任务可以更快的完成。设置属性：set hive.exec.parallel=true;
9、hive提供的严格模式，禁止3种情况下的查询模式。
a：当表为分区表时，where字句后没有分区字段和限制时，不允许执行。
b：当使用order by语句时，必须使用limit字段，因为order by 只会产生一个reduce任务。
c：限制笛卡尔积的查询。
10、合理的设置map和reduce数量。
11、jvm重用。可在hadoop的mapred-site.xml中设置jvm被重用的次数。

{% endhighlight %}

	

			

