---
layout: post
title: 2015-09-16-hive【五】hive综合案例之日志分析
categories:
- 设计模式
tags:
- 设计模式
---
[toc]
## hive综合案例之日志分析

### 作业提交

**hadoop作业提交:**

    hadoop jar $home/03ws/sp_intel/out/artifacts/hadoop_behavior_jar/hadoop_behavior.jar com.landray.behavior.job.JobManager
    
    hadoop jar /$home/03ws/sp_intel/hadoop_behavior/target/hadoop_behavior.jar com.landray.behavior.job.JobManager

## hive常用命令

### hiveserver2和beeline客户端

#### 服务端启动

    hive –service hiveserver2或者直接：hiveserver2

#### 客户端连接

**远程连接：**
    
    需要hive和hadoop复制客户端机器，并配置好变量:   
    $ beeline -u jdbc:hive2://master:10000 -n nemo -p tan -e "select count(*) from request_log"  
**beelilne客户端链接：**  

    输入beeline后进入beeline shell：!connect jdbc:hive2://localhost:10000### hive jdbc客户端开发环境搭建步骤
#### jdbc连接hive
1、创建一个mavenmodule  
2、引入必要jar包

{% highlight java %}      
<dependency>  
    <groupId>org.apache.hadoop</groupId>  
    <artifactId>hadoop-common</artifactId>  
    <version>2.6.0</version>  
</dependency>  
<dependency>  
    <groupId>org.apache.hive</groupId>  
    <artifactId>hive-jdbc</artifactId>  
    <version>1.2.1</version>  
</dependency> 
{% endhighlight %}

3、jdbc连接

    try {  
             Class.forName(driverName);  
        } catch (ClassNotFoundException e) {  
            // TODO Auto-generated catch block  
            e.printStackTrace();  
            System.exit(1);  
        }  
        //replace "hive" here with the name of the user the queries should run as  
        Connection con = DriverManager.getConnection("jdbc:hive2://master:10000/default", "nemo", "lovelili");  
        Statement stmt = con.createStatement();  
        String tableName = "testHiveDriverTable";  
        stmt.execute("drop table if exists " + tableName);

### hive自定义函数UDAF实例

**参考：**[hive udaf开发入门和运行过程详解][1]

#### **函数常用命令**

{% highlight java %}   
#打包
javac -d Lower Lower.java  
jar -cvf collect_size.jar -C / .  
使用mvn打包：mvn package  
#临时注册  
create temporary function collect_set_size as 'com.landray.hive.ql.GenericUDAFCollectSetSize';  

create temporary function size_sum as 'com.landray.hive.ql.ArraySizeSum';  
#永久注册  
org.apache.hadoop.hive.ql.exec.FunctionRegistry源码见这个  
show functions  #显示所有的函数      
list jars       #显示所有的临时jars

{% endhighlight %}
### hive常用命令

**hive常用ddl：**

    netstat –apn | grep 10000  
    ps -aux | grep pid  
  
    show tables #显示所有的表  
    desc table_name #查询表结构  
    show partitions request_log; #显示分区信息  
    select url['server'] from request_log where dt='1797'; #查询map结构  
    select ref,size(ref)from request_log; #查询map类型的size可以判断size是否为空   
    select * from request_log limit 1; #top查询  
    !connect jdbc:hive2://localhost:10000 #belline客户端链接server端口      
    beeline -u jdbc:hive2://localhost:10000 -n nemo -p tan -e "show tables";#客户端连接

## 日志分析设计

### hive表设计

**方案一：** 每一天存放一个目录，每天生成一个日志文件（所有用户），并且跨天的日志

* request_transform   

  * 2015-09   

    * 2015-09-07（文件传输时间）

**方案二：**每个用户一个主文件夹，会导致小文件很多，好维护需要每个月归档，但是有个问题就是面对跨天的问题，归档会错乱

* user_id   

  * month   

    * day

hive创建外部表：

{% highlight java %}
CREATE EXTERNAL TABLE IF NOT EXISTS request_log(  
file_date STRING,  
id STRING,  
job_create BIGINT,  
node STRING,  
session_id STRING,  
time BIGINT,  
ip STRING,  
user_id STRING,  
name STRING,  
ua INT,  
browser STRING,  
browser_ver STRING,  
log_type STRING,  
dt BIGINT,  
url MAP<STRING,STRING>,  
ref MAP<STRING,STRING>)  
PARTITIONED BY (month STRING,day STRING)  
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'  
COLLECTION ITEMS TERMINATED BY ','  
MAP KEYS TERMINATED BY '::'  
LOCATION '/output/logs/request_transform/';  
{% endhighlight %}

增加分区：

{% highlight java %}
_month='2015-09'  
_date='2015-09-07'  
hive -e "ALTER TABLE request_log ADD PARTITION(month='${_month}',day='${_date}') LOCATION '/output/logs/request_tra  
nsform/${_month}/${_date}';"
{% endhighlight %}

### 日志记录步骤：

#### 原始日志

    1（标记）   6F1F682FD889EAEE10EEF84769CBD0F9（服务器段sessionId） 1432137603417(记录日志|请求之后的当前时间)   183.13.217.114(ip地址)    1271da54250e08909144e724b32b689c（用户Id）  %E7%8E%8B%E5%8D%8E(用户名加密)   6（客户端类型）【浏览器】 【浏览器版本】           html【contentType】   /sys/task/sys_task_main/sysTaskMain.do?method=addChildTask&fdTaskId=14d1e27ce81307c522a09014402a1102&flag=  /sys/task/sys_task_main/sysTaskMain.do?method=view&fdId=14d1e27ce81307c522a09014402a1102&_mobile=1【URI并加上参数】【refer url】 1797【时间差】

#### 清理日志

    file_year               string          文件年  
    file_month              string          文件月  
    file_date               string          文件day  
    id                      string          项目Id  
    job_create              bigint          job创建时间  
    node                    string          根据文件名获取nodename  
    session_id              string          sessionId  
    time                    bigint          记录时间-时间差=请求开始时间  
    ip                      string          ip地址  
    user_id                 string          userid  
    name                    string          用户名  
    ua                      int             MobileUiil中的UA  
    browser                 string          根据UA得出的浏览器  
    browser_ver             string          根据UA得出的浏览器版本  
    log_type                string          contentTYPE得出的类型  
    dt                      bigint          请求时间差  
    url                     map<string,string>  url清洗之后  
    ref                     map<string,string>  ref清洗之后  
    isModule                    boolean     清洗，判断日志是否可以  
    month                   string          分区|日志调度month  
    day                     string          分区|日志调度day

## 实现增量分析(以portal_use为例)

**背景：**每天上传到服务器的日志，经过**每日增量分析**得到当天的结果，然后合并更新到总结果集。并且只把更新的数据导入到mongodb结果数据库。

{% highlight java %}
sh portal_use file_month day 2015-09-07
{% endhighlight %}
1、初始化创建一个结果集表res_portal_use，以用户ID和维度分区。LAST_UPDATE表示该行数据**最后更新时间**。

    CREATE TABLE IF NOT EXISTS RES_PORTAL_USE(  
    FDID STRING,  
    COUNT BIGINT,  
    LAST_UPDATE DATE  
    ）  
    PARTITIONED BY (ID STRING,SCOP STRING)  
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';  
2、当天的结果表：tmp_portal_use_20151104,其中LAST_UPDAYET为今天的日期。对flume上传到hdfs上的今天的日志进行分析，得出今天的结果集。

3、两个表中的字段是一样的，union all之后进行sum(count),max(LAST_UPDATE)就是增量之和以及LAST_UPDATE为今天的是要导入到mongodb的结果集。该结果集放到tmp_portal_use表中

    INSERT OVERWRITE TABLE TMP_RES_portal_use PARTITION(ID,SCOP) SELECT sub.FDID AS FDID,SUM(sub.COUNT) AS COUNT,MAX(sub.LAST_UPDATE) AS LAST_UPDATE,sub.ID as ID,sub.SCOP as SCOP FROM (SELECT fdid,count,to_date('2015-11-05') as LAST_UPDATE,id,scop FROM tmp_portal_use_20151105 UNION ALL SELECT * FROM RES_portal_use)sub GROUP BY sub.ID,sub.SCOP,sub.FDID  
> 注意：该处需要用到动态分区。

4、导入之后，先删除旧的res_portal_use表，再把tmp_portal_use表重命名为res_portal_use表。那么res_portal_use就变成了最新的结果集表。

    #-------------导入并归档新结果集------  
 
 {% highlight java %}
 28 #移除old结果集  
 29 hive -e "DROP TABLE tmp_$0_${_date}"  
 30 hive -e "DROP TABLE RES_$0"  
 31 #重命名新结果集  
 32 hive -e "ALTER TABLE TMP_RES_$0 RENAME TO RES_$0"  
 33 #导入更新的结果  
 {% endhighlight %}
5、把res_portal_use表中LAST_UPDATE为今天的的数据更新导入到mongodb中。

## top监控

**未执行之前，6个java进程分别是：hadoop自身进程和hive client进程**  

## hive和mongodb map-reduce耗时比较

### 用到的hadoop常用命令

**hadoop-2.6.0版本**
{% highlight java %}
$ hadoop fs -ls /output/logs/request_transform         #查询  
$ hadoop fs -mkdir /output/logs/request_transform/2015-09/2015-09-08 #创建一个目录  
$ hadoop fs -copyFromLocal ./100W.txt /output/logs/request_transform/2015-09/2015-09-08        #上传到hdfs  
$ hadoop fs -copyToLocal       #下载到本地  
$ hadoop fs -rmr /output/logs/request_transform/2015-09/2015-09-08  
$ mapred job -list             #显示当前的job  
$ mapred job -kill jobid        #killjiob
{% endhighlight %}

### 构建数据脚本

**此处先给出构建数据的脚本**

#### 构建hive数据

>**思路：** 利用脚本把原始数据复制N倍大到数据量。

1、原始数据保存在hdfs上，先下载到linux环境上。利用hadoop dfs -copyToLocal把文件copy到本地。

{% highlight java %}
#head -n 10000 log.txt >> 1W.tx        #取W条记录  
#more 1W.txt | wc -l                   #计数  
#du -h                                 #查看文件大小  
#du -m --max-depth=1  /etc | sort -nr  #查看文件夹大小  
#du --max-depth=1 -h                   #当前目录  
#ls -l --block-size=M                  #查看文件大小  
{% endhighlight %}

**构建大文本脚本**
{% highlight java %}
  1 #/bin/bash  
  2 if [ $# -ne 2 ]  
  3    then  
  4      echo "第一个参数为写多少次，第二个参数问生成的文件名"  
  5      return  
  6 fi  
  7 count=$1  
  8 file=$2  
  9  
 10 while [ $count -gt 0 ]  
 11    do  
 12      count=$(($count-1))  
 13      `cat 1W.txt >> $file`  
 14    done  
 15
{% endhighlight %}
#### 构建mongodb数据

### 数据测试


|条数(万条)|大小|M-R：耗时(S)|总耗时(S)||mongodb耗时(S)|
|---|---|---|--|--|--|
|50|242|2-1:16|120|
|100|484|2-1:38|168|
|100|484|4-1:34|156|
|500|2418|4-1:65|176|
|1000|4836|4-1:86|206|

>**注意：** 由于是虚拟里中伪分布式环境，当map和reduce个数超过2以上的时候，执行会非常缓慢，因为从top命令中可以看出，只启动了2个jvm进行M-R调度。多个M-R起不到并行计算的效果。

### 遇到的问题

#### 问题一：测试数据大的时候后，job一致处于appending状态

>利用jps查看，发现nodemanager都被干掉了，于是先stop-yan.sh然后start-yarn.sh> 重启。

#### 问题二：内存不够的问题

**原因：**
>因为hadoop默认每个map任务和reduce任务默认的内存分配是1024，所以当分配的总内存>计算机的内存的时候则会出现问题。   

**解决：**  
修改**mapred-site.xml文件:** 

{% highlight java %}
mapreduce.map.memory.mb  
mapreduce.reduce.memory.mb
set mapreduce.input.fileinputformat.split.maxsize=1024000;   
set yarn.timeline-service.handler-thread-count=2;
{% endhighlight %}


#### 问题三：如何控制map个数

**方案一:** 设置分片大小   

{% highlight java %}
mapreduce.input.fileinputformat.split.minsize  #默认为1   
mapreduce.input.fileinputformat.split.maxsize  #默认为256M   
dfs.block.size #块大小
#计算分片大小的公式
  protected long computeSplitSize(long blockSize, long minSize,
                                  long maxSize) {
    return Math.max(minSize, Math.min(maxSize, blockSize));
  }
  
  
在hadoop中blocksize：128M，为什么hive中map数目以256M为一个split进行分割呢？因为：
参数：set mapred.max.split.size = 256000000 ; //最大分割
该参数是hive进行map端分割的参数
{% endhighlight %}

**方案二：**

## 暴露的问题

{% endhighlight %}
1、job提交前宕机问题  
2、hive并行，shell并发进程卡机问题  
3、hive数据倾斜问题  

count(distinct)性能问题分析   
viewspace-1262946/   
hive.map.aggr=true   
hive.groupby.skewindata=true   
4、多job运行问题
{% endhighlight %}


## 附录

### hadoop、hive以及hive常用的命令：

{% highlight java %}
hadoop jar $home/03ws/sp_intel/out/artifacts/hadoop_behavior_jar/hadoop_behavior.jar com.landray.behavior.job.JobManager #hadoop提交作业  
  
jar xf xxx.jar   #解压jar包 输入jar命令会有详细提示  
jar cvf hive_behavior.jar com/ #把包下所有的class文件打包，com也打进去  
sudo du -shm /Users/nemo/Library/* | sort -nr #查看文件夹下的大小  
sh module.sh file_month month 2015-09-07 #执行某个  
{% endhighlight %}

**创建表语句**

{% highlight java %}
CREATE table student_map as select * from student;  
CREATE TABLE IF NOT EXISTS student_map(id MAP<STRING,STRING>,value MAP<STRING,STRING>)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'; #创建MAP类型的表  
insert into student_map as 
{% endhighlight %}

### vim常用命令

{% highlight java %}
set nu #显示行号  
$ #定位到行末尾  
0 #定位到行首  
nl #移动到n列之后
{% endhighlight %}

### 苹果系统清理bash

{% highlight java %}
sudo rm -fr /Users/nemo/Library/Caches #清理应用缓存文件  
sudo rm -fr /Users/nemo/Library/Containers/com.tencent.qq #移除qq缓存
{% endhighlight %}

### bak

{% highlight java %}
#执行hive  
  hive -e "$HQL_SYS_ALL\  
           UNION ALL $HQL_PC\  
           UNION ALL $HQL_PC_BRO\  
           UNION ALL $HQL_PC_BRO_VER\  
           UNION ALL $HQL_MOBILE\  
           UNION ALL $HQL_PHONE\  
           UNION ALL $HQL_ENTRY\  
           UNION ALL $HQL_ENTRY_TYPE\  
           "
{% endhighlight %}
### mongodb常用查询语句

{% highlight java %}
db.getCollection('count_module_201505').find({"_id.type":"module"},{"_id.module":1})  
  
select ref['module'],ref['path'],url['method'] from request_log where url['module'] is not null       and (log_type is null or log_type='html')        and url['portlet']='false'        and check(url['module'],url['path'],url['method'])=true and day='2015-09-07';  
  
select distinct url['module'],url['path'],url['method'] from request_log where url['module'] is not null       and (log_type is null or log_type='html')        and url['portlet']='false'        and check(url['module'],url['path'],url['method'])=true and day='2015-09-07';

{% endhighlight %}
### 坑

    1、count_module查询的时候不准确  
    --过滤数据null判断要判断字符串  
    --要忽略大小写判断

### hive调优

    1、join连接时的优化：当三个或多个以上的表进行join操作时，如果每个on使用相同的字段连接时只会产生一个mapreduce。  
    2、join连接时的优化：当多个表进行查询时，从左到右表的大小顺序应该是从小到大。原因：hive在对每行记录操作时会把其他表先缓存起来，直到扫描最后的表进行计算  
    3、在where字句中增加分区过滤器。  
    4、当可以使用left semi join 语法时不要使用inner join，前者效率更高。原因：对于左表中指定的一条记录，一旦在右表中找到立即停止扫描。  
    5、如果所有表中有一张表足够小，则可置于内存中，这样在和其他表进行连接的时候就能完成匹配，省略掉reduce过程。设置属性即可实现，set hive.auto.covert.join=true; 用户可以配置希望被优化的小表的大小 set hive.mapjoin.smalltable.size=2500000; 如果需要使用这两个配置可置入$HOME/.hiverc文件中。  
    6、同一种数据的多种处理：从一个数据源产生的多个数据聚合，无需每次聚合都需要重新扫描一次。  
    例如：insert overwrite table student select *　from employee; insert overwrite table person select * from employee;  
    可以优化成：from employee insert overwrite table student select * insert overwrite table person select *  
    7、limit调优：limit语句通常是执行整个语句后返回部分结果。set hive.limit.optimize.enable=true;  
    8、开启并发执行。某个job任务中可能包含众多的阶段，其中某些阶段没有依赖关系可以并发执行，开启并发执行后job任务可以更快的完成。设置属性：set hive.exec.parallel=true;  
    9、hive提供的严格模式，禁止3种情况下的查询模式。  
    a：当表为分区表时，where字句后没有分区字段和限制时，不允许执行。  
    b：当使用order by语句时，必须使用limit字段，因为order by 只会产生一个reduce任务。  
    c：限制笛卡尔积的查询。  
    10、合理的设置map和reduce数量。  
    11、jvm重用。可在hadoop的mapred-site.xml中设置jvm被重用的次数。

